{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all the packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a list of years which you have data for\n",
    "years = [str(x) for x in range(1965,2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The url you need to append the data to.\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hit all the year links and get the html content and put em in a dictionary where years are your keys\n",
    "def hit_em_all():\n",
    "    year_wise_html = {}\n",
    "    for year in years:\n",
    "        url = wiki_url+year\n",
    "        result = requests.get(url)\n",
    "        content = result.content\n",
    "        year_wise_html[year] = BeautifulSoup(content, 'html.parser')\n",
    "    return(year_wise_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call em all\n",
    "hot_100s = hit_em_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the table containing the hot 100 from each html (of the respective year)\n",
    "def get_song_list_table(table):\n",
    "    table_class = table.attrs['class']\n",
    "    table_class_list = []\n",
    "    for class_name in table_class:\n",
    "        #print(class_name)\n",
    "        # this extracts the class name of the table. It is of type unicode.\n",
    "        # Hence, it has to be encoded using ascii to convert to a String\n",
    "        table_class_list.append(class_name.encode('ascii','ignore'))\n",
    "    class_name = \" \".join(table_class_list)\n",
    "    return(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from all the tables in the page, find out the one with the class name 'wikitable sortable'\n",
    "def get_the_table(page):\n",
    "    tables = page.findChildren('table')\n",
    "    for table in tables:\n",
    "        if get_song_list_table(table) == 'wikitable sortable':\n",
    "            return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for each year where year is the key and the table becomes the value\n",
    "def get_all_year_song_tables():\n",
    "    year_wise_table = {}\n",
    "    for year, page in hot_100s.items():\n",
    "        year_wise_table[year] = get_the_table(page)\n",
    "    return(year_wise_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_wise_table = get_all_year_song_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(year_wise_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Datapipeline\n",
    "# Once you have the table, you need to push each of them into our data pipeline and extract the \n",
    "# song and artist links.. and then hit the song links and extract the table containing the genre info/\n",
    "# and finally extract the name  of the genres.\n",
    "def data_pipeline(year):\n",
    "    # get year as an input and convert that to string\n",
    "    year = ''+str(year)\n",
    "    # index your dictionary with the input year and extract all td elements from the table\n",
    "    all_tds = year_wise_table[year].find_all('td')\n",
    "\n",
    "    my_tds = []\n",
    "    for td in all_tds:\n",
    "        # if the text of the td element is a number, ignore it\n",
    "        if not td.text.isdigit():\n",
    "            # the rest you append to our list\n",
    "            my_tds.append(td)\n",
    "\n",
    "    # artist links and song links are alternatively placed as td elements. So take them apart!\n",
    "    artist_links = my_tds[1::2]\n",
    "    song_links = my_tds[::2]\n",
    "\n",
    "    # Get links to the  artists\n",
    "    def get_full_artist_links():\n",
    "        all_artists = []\n",
    "        for artist in artist_links:\n",
    "            if not artist.text.isdigit():\n",
    "                all_artists.append(artist.text.encode('ascii','ignore'))\n",
    "        return(all_artists)\n",
    "\n",
    "    artists = get_full_artist_links()\n",
    "\n",
    "    # Get song links along with song names\n",
    "    def get_full_song_links():\n",
    "        full_song_links = []\n",
    "        song_name_list = []\n",
    "        wikipedia = 'https://en.wikipedia.org'\n",
    "        for song in song_links:\n",
    "            #print(song)\n",
    "            # song info is embedded inside anchor tags in the td elements\n",
    "            a = song.find('a')\n",
    "            #print(a)\n",
    "            if a:\n",
    "                # get the href element from your anchor tags\n",
    "                link = a.attrs['href']\n",
    "                # get the text from the anchor tag.. which is your song's name\n",
    "                song_name_list.append(a.text.encode('ascii','ignore'))\n",
    "                # extract the link from the href tag and append it to the wiki url\n",
    "                full_song_links.append(wikipedia+link.encode('ascii','ignore'))\n",
    "        return(full_song_links,song_name_list)\n",
    "\n",
    "    links, songs = get_full_song_links()\n",
    "\n",
    "    # Now, we have three lists -> songs (name of songs), artists(name of artists), links(links to the songs)\n",
    "    # We are gonna create a df out of them with year as the first column.\n",
    "    # We are gonna create a year list with the length of the smallest list out of artists, songs and links\n",
    "    table = pd.DataFrame(list(zip([1990]*min(len(artists),len(songs),len(links)),songs, artists, links)), columns=['Year','Song','Artist','Song Link'])\n",
    "\n",
    "    table.head(5)\n",
    "    \n",
    "    # Get the table containing the genre info\n",
    "    def get_the_right_table(table):\n",
    "        attributes = table.attrs\n",
    "        table_class_list = []\n",
    "        class_name=''\n",
    "        if 'class' in attributes:\n",
    "            table_classes = table.attrs['class']\n",
    "            for class_name in table_classes:\n",
    "                table_class_list.append(class_name.encode('ascii','ignore'))\n",
    "            class_name = \" \".join(table_class_list)\n",
    "            return(class_name)\n",
    "\n",
    "    \n",
    "    def get_genres(table):\n",
    "        genre_dict = dict()\n",
    "        songs = []\n",
    "        for index, row in table.iterrows():\n",
    "            #print('Song',row['Song'])\n",
    "            #print('Artist',row['Artist'])\n",
    "            try:\n",
    "                # hit the song links and get the html content\n",
    "                result = requests.get(row['Song Link'])\n",
    "                content = result.content\n",
    "                html = BeautifulSoup(content, 'html.parser')\n",
    "                # get all tables from the  html\n",
    "                tables = html.findChildren('table')\n",
    "                my_table = ''\n",
    "                for table in tables:\n",
    "                    # if the table name is 'infobox vevent', then thats the one that contains the genre info\n",
    "                    if get_the_right_table(table) == \"infobox vevent\":\n",
    "                        my_table = table\n",
    "                        # one page could have multiple tables with the same class name.\n",
    "                        # cos many artists have covered the same song in different styles.\n",
    "                        # Hence, we need to get rid of those genres and stick with the original genre info\n",
    "                        # So we are breaking the loop here cos we are not gonna look\n",
    "                        # at any more tables (if any), to get more genre info\n",
    "                        break\n",
    "                # from that table, get all anchor tags\n",
    "                # from that table, get all anchor tags\n",
    "                all_as = my_table.find('td',{'class':'category hlist'})\n",
    "                a = all_as.text.encode('ascii','ignore')\n",
    "                # remove square brackets and digits if any\n",
    "                pattern = r'\\[.*?\\]'\n",
    "                s = re.sub(pattern, '', a)\n",
    "                genres = re.sub(r'\\d+', '', s)\n",
    "                #print(genres)\n",
    "                key = row['Song']\n",
    "                genre_dict[key] = genres\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "        return(genre_dict)\n",
    "\n",
    "    #if hasNumbers(a.text.encode('ascii','ignore'))\n",
    "    genre_dict = get_genres(table)\n",
    "\n",
    "    #print(genre_dict)\n",
    "\n",
    "    # Now, we have the genre dictionary, extract them all and split list of lists into seperate items.\n",
    "\n",
    "    # Create a df with songs and genres lists\n",
    "    df_genres = pd.DataFrame(list(genre_dict.iteritems()),\n",
    "                      columns=['Song','Genres'])\n",
    "\n",
    "    #print(df_genres.head(5))\n",
    "\n",
    "    return(df_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = range(1965,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automate the entire thing.\n",
    "for y in year:\n",
    "    df = data_pipeline(y)\n",
    "    # Write the output to csv file for each year.\n",
    "    a = str(y)+'.csv'\n",
    "    df.to_csv(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
